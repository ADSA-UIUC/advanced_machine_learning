{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ADSA Logo](http://i.imgur.com/BV0CdHZ.png?2 \"ADSA Logo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spring 2018 ADSA Workshop - Python Series: Advanced Machine Learning\n",
    "> Workshop content by ADSA with some adaptations from: \n",
    "> http://scikit-learn.org/stable/user_guide.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Welcome to the Advanced Machine Learning workshop. Today we will go over:\n",
    "* Review of K Nearest Neighbor\n",
    "* Decision Trees \n",
    "* Naive Bayes\n",
    "\n",
    "\n",
    "To run any block of code, type `Ctrl + ENTER`. This will execute the code and tell you the errors encountered, if there were any.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knn Image from Wikipedia][knni]\n",
    "[knni]: https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/500px-KnnClassification.svg.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by importing our iris data set(https://en.wikipedia.org/wiki/Iris_flower_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset.feature_names)\n",
    "print(iris_dataset['data'][::50])\n",
    "print(iris_dataset.target_names)\n",
    "print(iris_dataset['target'][::50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the train_test_split function, which will split our data into two subsets: training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    iris_dataset['data'], iris_dataset['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need to split the data into training and testing data? \n",
    "In machine learning, we generalize patterns beyond the data used to train models. Because we do not know the target values, we can not check the accuracy of our predictions in the future. Therefore, we can use a portion of training data to test our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "prediction = knn.predict(X_new)\n",
    "print(\"Prediction: {}\".format(prediction))\n",
    "print(\"Predicted target name: {}\".format(\n",
    "       iris_dataset['target_names'][prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are one of the few models that allow users to interpret exactly why the classifier makes a decision. Under the hood, the decision tree classifer asks a series of yes or no questions. Based on the answers, the classifier will return its decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    iris_dataset['data'], iris_dataset['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "prediction = clf.predict(X_new)\n",
    "print(\"Prediction: {}\".format(prediction))\n",
    "print(\"Predicted target name: {}\".format(\n",
    "       iris_dataset['target_names'][prediction]))Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "import pydotplus\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "                                feature_names=iris_dataset.feature_names,\n",
    "                                class_names=iris_dataset.target_names,\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_pdf('iris.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise: Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data using the train_test_split. Set random_state to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the data using the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the class of a wine_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the score of the decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(wine_clf, out_file=None,\n",
    "                                feature_names=wine_dataset.feature_names,\n",
    "                                class_names=wine_dataset.target_names,\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_pdf('wine.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Naive Bayes][nb] \n",
    "\n",
    "The first algorithm we will explore is one of the simplest, the [Naive\n",
    "Bayes][nb] algorithm. Fundamentally, this algorithm is remarkably simple\n",
    "and is based on the assumption of independence that a given attribute (or\n",
    "feature) belongs to a specific class. Scikit learn provides several\n",
    "Naive Bayes implementations, using a specific one generally depends on\n",
    "the nature of the data being analyzed:\n",
    "\n",
    "- [Gaussian Naive Bayes][gnb] \n",
    "- [Multinomial Naive Bayes][mnb] \n",
    "- [Bernoulli Naive Bayes][bnb] \n",
    "\n",
    "In the following code cells, we demonstrate how to perform Naive Bayes\n",
    "classification by using scikit learn, in this case we use the Gaussian\n",
    "Naive Bayes implementation. The standard classification process in\n",
    "scikit learn is to first fit a model to the training data and to\n",
    "subsequently apply this model to predict values for the testing data.\n",
    "After this process, we first compute the prediction score before\n",
    "displaying the confusion matrix for this algorithm.\n",
    "\n",
    "-----\n",
    "[gnb]: http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\n",
    "[mnb]: http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\n",
    "[bnb]: http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n",
    "[nb]: https://en.wikipedia.org/wiki/Naive_Bayes_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten Digits\n",
    "\n",
    "We will explore the Naive Bayes\n",
    "classification applied to handwritten digits. First we will load the\n",
    "data and review some of the sample. After which, we will apply a\n",
    "Multinomial Naive Bayes classification to this data and explore the\n",
    "resulting predictions.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have helper code to assist in the loading and plotting of these data\n",
    "import digits as hd\n",
    "\n",
    "# We extract features (x), labels (y), and the images for plotting.\n",
    "x, y, images = hd.get_data()\n",
    "hd.im_plot(x, y, images)\n",
    "\n",
    "print('Total number of samples = {0}'.format(x.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Classification of Image Data\n",
    "\n",
    "Now that these data are loaded, we can apply a Naives Bayes classifier\n",
    "to this problem. Below we employ a Multinomial Naive Bayes model, and\n",
    "show the classification score, and the classification report.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cross_validation as cv\n",
    "(x_train, x_test, y_train, y_test) = cv.train_test_split(x, y, test_size=.25)\n",
    "\n",
    "# First, lets try Gaussain NB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# First we construct our Classification Model\n",
    "nbc = MultinomialNB()\n",
    "\n",
    "nbc.fit(x_train, y_train);\n",
    "\n",
    "print('Prediction Accuracy = {0:3.1f}%'.format(100*nbc.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more information on the precision, recall, f1-score, and support is found here: \n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = nbc.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "The Naive Bayes model performed in a reasonable manner, but what about\n",
    "on completely blind data. We can test the model by _making_ new data.\n",
    "Below we use methods in the helper code to make _fake_ data, in this\n",
    "case images of **one** and **seven**. We display the _fake_ images, and\n",
    "then the results of our model classifying these new data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = hd.make_ones()\n",
    "hd.plot_numbers(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual : Predicted')\n",
    "for one in ones:\n",
    "    print('  1    :     {0}'.format(nbc.predict([one])[0])) # You can use \"one\" instead of \"[one]\"\n",
    "                                                            # But it's depreciated in newer versions of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test on our sevens\n",
    "sevens = hd.make_sevens()\n",
    "hd.plot_numbers(sevens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual : Predicted')\n",
    "for seven in sevens:\n",
    "    print('  7    :     {0}'.format(nbc.predict([seven])[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
